{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e15f9479-12db-46a2-b767-8547f37e4bea",
   "metadata": {},
   "source": [
    "## Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d9e1dc-84e4-4804-b0ae-88df03c2109a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Min-Max scaling, also known as normalization, is a data preprocessing technique used to transform numerical features to a common scale, \n",
    "typically between 0 and 1. This technique is useful when features have different ranges or units, which can potentially affect the performance \n",
    "of certain machine learning algorithms that are sensitive to the scale of input features.\n",
    "\n",
    "The formula for Min-Max scaling is as follows:\n",
    "\n",
    "\n",
    "Xscaled=x−min(x)/(max(x)−min(x))\n",
    "\n",
    "where:\n",
    "\n",
    "x is the original feature value.\n",
    "min(x) is the minimum value of the feature across the dataset.\n",
    "max(x) is the maximum value of the feature across the dataset.\n",
    "Xscaled is the scaled feature value between 0 and 1.\n",
    "\n",
    "Here's an example to illustrate the application of Min-Max scaling:\n",
    "\n",
    "Suppose you have a dataset of house prices with two features: \"Size\" (in square feet) and \"Age\" (in years). The original values of these features\n",
    "are as follows:\n",
    "\n",
    "Size: [1500, 2000, 1800, 2200, 1600]\n",
    "Age: [5, 10, 2, 15, 8]\n",
    "To apply Min-Max scaling:\n",
    "\n",
    "Calculate Minimum and Maximum Values:\n",
    "\n",
    "Minimum Size: 1500\n",
    "Maximum Size: 2200\n",
    "Minimum Age: 2\n",
    "Maximum Age: 15\n",
    "Apply Min-Max Scaling Formula:\n",
    "\n",
    "For the \"Size\" feature, apply the Min-Max scaling formula:\n",
    "\n",
    "Scaled Size = (Size - Minimum Size) / (Maximum Size - Minimum Size)\n",
    "Scaled Size = (1500 - 1500) / (2200 - 1500) = 0\n",
    "Scaled Size = (2000 - 1500) / (2200 - 1500) = 0.5\n",
    "Scaled Size = (1800 - 1500) / (2200 - 1500) = 0.25\n",
    "Scaled Size = (2200 - 1500) / (2200 - 1500) = 1\n",
    "Scaled Size = (1600 - 1500) / (2200 - 1500) = 0.125\n",
    "For the \"Age\" feature, apply the Min-Max scaling formula:\n",
    "\n",
    "Scaled Age = (Age - Minimum Age) / (Maximum Age - Minimum Age)\n",
    "Scaled Age = (5 - 2) / (15 - 2) = 0.1875\n",
    "Scaled Age = (10 - 2) / (15 - 2) = 0.75\n",
    "Scaled Age = (2 - 2) / (15 - 2) = 0\n",
    "Scaled Age = (15 - 2) / (15 - 2) = 1\n",
    "Scaled Age = (8 - 2) / (15 - 2) = 0.5\n",
    "\n",
    "After applying Min-Max scaling, your scaled feature values are in the range of [0, 1], making them more suitable for machine learning algorithms \n",
    "that require standardized input features.\n",
    "\n",
    "Keep in mind that while Min-Max scaling can help in some cases, it might not be appropriate for features with outliers or when you want to \n",
    "preserve the relationships between data points. In such cases, you might consider using other scaling techniques like Z-score normalization or \n",
    "robust scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63567aff-cb76-433d-9f43-6d3be26f32c5",
   "metadata": {},
   "source": [
    "## Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ddd1db-2aec-4dae-8535-22c896398de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "The Unit Vector technique, also known as Vector Normalization, is a feature scaling method that transforms numerical features to have a \n",
    "length of 1 while preserving their direction. This technique is particularly useful when you want to ensure that the magnitude of each feature \n",
    "vector doesn't affect the performance of algorithms that rely on the distances or dot products between vectors.\n",
    "\n",
    "Unlike Min-Max scaling, which scales features within a specific range (e.g., [0, 1]), Unit Vector scaling focuses on the direction of the vectors.\n",
    "The formula for Unit Vector scaling is as follows:\n",
    "\n",
    "x unit= x/∥x∥\n",
    "\n",
    "where:\n",
    "\n",
    "x is the original feature vector.\n",
    "∥x∥ is the Euclidean norm (length) of the feature vector.\n",
    "x unit is the unit vector version of the feature.\n",
    "\n",
    "Here's an example to illustrate the application of Unit Vector scaling:\n",
    "\n",
    "Suppose you have a dataset of data points in two dimensions: (x,y). The original values of these data points are as follows:\n",
    "\n",
    "Data Point 1: (3, 4)\n",
    "Data Point 2: (1, 2)\n",
    "Data Point 3: (6, 8)\n",
    "To apply Unit Vector scaling:\n",
    "\n",
    "Calculate the Euclidean Norm:\n",
    "\n",
    "Calculate the Euclidean norm (∥x∥) for each data point using the formula ||x||=sqrt(x**2 + y**2)\n",
    "Norm for Data Point 1 |x|: sqrt(3**2 + 4**2) = 5\n",
    "Norm for Data Point 2 |x|: sqrt(1**2 + 2**2)= sqrt(5)\n",
    "Norm for Data Point 3 |x|: sqrt(6**2 + 8**2) = 10\n",
    "\n",
    "Apply Unit Vector Scaling Formula:\n",
    "\n",
    "For each data point, apply the Unit Vector scaling formula:\n",
    "\n",
    "Unit Vector = x/|x|\n",
    " \n",
    "Unit Vector for Data Point 1:\n",
    "\n",
    "x unit = 3/5\n",
    "y unit = 4/5\n",
    "\n",
    " \n",
    "Unit Vector for Data Point 2:\n",
    "\n",
    "x unit = 1/sqrt(5)\n",
    "y unit = 2/sqrt(5)\n",
    " \n",
    "Unit Vector for Data Point 3:\n",
    "\n",
    "x unit = 6/10\n",
    "y unit = 8/10\n",
    " \n",
    "After applying Unit Vector scaling, each data point's vector length is 1, preserving its direction. This normalization can be useful in machine \n",
    "learning algorithms where the distance or dot product between vectors matters more than their individual magnitudes.\n",
    "\n",
    "Keep in mind that Unit Vector scaling doesn't handle differences in magnitudes between features as directly as Min-Max scaling. It focuses solely\n",
    "on preserving the direction of vectors, which can be advantageous in certain scenarios where feature direction matters more than their magnitudes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e06770e-31e6-4867-af6d-df2dd57a2768",
   "metadata": {},
   "source": [
    "## Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce156648-eb26-4868-8714-1d9a43bf580f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Principal Component Analysis (PCA) is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional \n",
    "space while preserving as much of the original data's variance as possible. It achieves this by identifying the principal components, which \n",
    "are orthogonal linear combinations of the original features. These components capture the directions of maximum variance in the data.\n",
    "\n",
    "The key idea behind PCA is to project the data onto a new coordinate system defined by the principal components. The first principal component \n",
    "corresponds to the direction of maximum variance, the second principal component is orthogonal to the first and captures the next most significant\n",
    "variance, and so on.\n",
    "\n",
    "Here's how PCA is used in dimensionality reduction:\n",
    "\n",
    "Standardize the Data:\n",
    "\n",
    "    Scale the data to have zero mean and unit variance across all features.\n",
    "\n",
    "Calculate the Covariance Matrix:\n",
    "\n",
    "    Compute the covariance matrix of the standardized data. The covariance matrix provides information about how the features correlate with \n",
    "    each other.\n",
    "\n",
    "Calculate Eigenvectors and Eigenvalues:\n",
    "\n",
    "    Calculate the eigenvectors and eigenvalues of the covariance matrix. Eigenvectors represent the principal components, and eigenvalues\n",
    "    indicate the amount of variance explained by each principal component.\n",
    "\n",
    "Sort Eigenvectors by Eigenvalues:\n",
    "\n",
    "    Sort the eigenvectors in descending order based on their corresponding eigenvalues. This helps identify the most important principal \n",
    "    components.\n",
    "\n",
    "Select Principal Components:\n",
    "\n",
    "    Decide how many principal components to retain based on the desired dimensionality reduction and the explained variance threshold. \n",
    "    Retaining fewer components reduces dimensionality but also introduces some loss of information.\n",
    "\n",
    "Transform the Data:\n",
    "\n",
    "    Project the original data onto the selected principal components to obtain the reduced-dimensional representation.\n",
    "\n",
    "Here's an example to illustrate PCA's application:\n",
    "\n",
    "Suppose you have a dataset with two features: \"Height\" and \"Weight\" of individuals. You want to reduce the dimensionality while retaining as \n",
    "much variance as possible.\n",
    "\n",
    "Original data:\n",
    "| Height (cm) | Weight (kg) |\n",
    "|-------------|-------------|\n",
    "| 170         | 65          |\n",
    "| 165         | 60          |\n",
    "| 180         | 75          |\n",
    "| 155         | 50          |\n",
    "| 175         | 70          |\n",
    "\n",
    "\n",
    "Standardize the Data:\n",
    "\n",
    "    Calculate the mean and standard deviation for each feature, then transform the data to have zero mean and unit variance.\n",
    "\n",
    "Calculate the Covariance Matrix:\n",
    "\n",
    "    Calculate the covariance matrix based on the standardized data.\n",
    "\n",
    "Calculate Eigenvectors and Eigenvalues:\n",
    "\n",
    "    Calculate the eigenvectors and eigenvalues of the covariance matrix.\n",
    "\n",
    "Sort Eigenvectors by Eigenvalues:\n",
    "\n",
    "    Sort the eigenvectors in descending order of eigenvalues.\n",
    "\n",
    "Select Principal Components:\n",
    "\n",
    "    Based on the eigenvalues, you might decide to retain both principal components.\n",
    "\n",
    "Transform the Data:\n",
    "\n",
    "    Project the original data onto the selected principal components to obtain the reduced-dimensional representation.\n",
    "\n",
    "The output could look something like this:\n",
    "| Principal Component 1 | Principal Component 2 |\n",
    "|-----------------------|-----------------------|\n",
    "| 0.816                 | -0.390                |\n",
    "| -0.341                | 0.383                 |\n",
    "| 1.125                 | 0.118                 |\n",
    "| -1.107                | -0.293                |\n",
    "| 0.507                 | 0.182                 |\n",
    "\n",
    "In this example, PCA has reduced the dimensionality from two features to two principal components while retaining the most significant \n",
    "information. The new representation can be used in subsequent analysis or modeling with reduced computational complexity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfff97f0-2053-44b8-a13c-238de08a3778",
   "metadata": {},
   "source": [
    "## Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08dd61a-6056-4329-951f-3c5b51dfcfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA (Principal Component Analysis) and feature extraction are closely related concepts in the context of dimensionality reduction and data \n",
    "representation. PCA can be used as a technique for feature extraction to transform high-dimensional data into a lower-dimensional space by \n",
    "creating new features that capture the most important information from the original features.\n",
    "\n",
    "Here's the relationship between PCA and feature extraction:\n",
    "\n",
    "PCA for Dimensionality Reduction:\n",
    "\n",
    "    PCA is often used to reduce the dimensionality of a dataset by identifying and retaining a subset of principal components that capture the \n",
    "    most significant variability in the data. This helps in simplifying the dataset and potentially improving computational efficiency and model \n",
    "    performance.\n",
    "\n",
    "Feature Extraction:\n",
    "\n",
    "    Feature extraction is a process that involves creating new features (also known as feature vectors) from the original features to represent \n",
    "    the data in a more compact and informative way. These new features aim to capture the most relevant and distinctive patterns in the data.\n",
    "\n",
    "PCA as a Feature Extraction Technique:\n",
    "\n",
    "    PCA can be used as a feature extraction technique because the principal components themselves can serve as the new features. These components \n",
    "    are linear combinations of the original features and are designed to maximize the variance captured by each component.\n",
    "\n",
    "Here's an example to illustrate how PCA can be used for feature extraction:\n",
    "\n",
    "    Suppose you have a dataset of images, where each image is represented by a high-dimensional vector of pixel values. Each pixel corresponds to \n",
    "    a feature, and the total number of pixels leads to a high-dimensional dataset. You want to reduce the dimensionality while preserving the most\n",
    "    important information in the images.\n",
    "\n",
    "Image Data:\n",
    "\n",
    "    You have a dataset of grayscale images, where each image is a 28x28 pixel grid, resulting in 784-dimensional feature vectors.\n",
    "\n",
    "PCA for Feature Extraction:\n",
    "\n",
    "    Apply PCA to the image dataset to identify the principal components that capture the most significant variability in the images.\n",
    "\n",
    "Reduced-Dimensional Features:\n",
    "\n",
    "    The principal components extracted by PCA can serve as the new features. Each principal component is a linear combination of the original \n",
    "    pixel values and represents a distinctive pattern in the images.\n",
    "\n",
    "Dimensionality Reduction:\n",
    "\n",
    "    By retaining only the top few principal components that explain a significant portion of the variance, you effectively reduce the \n",
    "    dimensionality of the image data.\n",
    "\n",
    "The outcome of this process is a reduced-dimensional representation of the images that retains the most important patterns. This transformed data \n",
    "can then be used for various tasks such as classification, clustering, or visualization.\n",
    "\n",
    "In summary, PCA can be seen as a feature extraction technique that creates new features (principal components) that capture the essential \n",
    "information from the original features. It's a powerful method for reducing dimensionality while maintaining the most significant aspects of \n",
    "the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebbe7f0-00f4-4fc3-b029-2e29105fbf77",
   "metadata": {},
   "source": [
    "## Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b9f8f2-6404-4d29-85b1-d4b581cb51df",
   "metadata": {},
   "outputs": [],
   "source": [
    "In building a recommendation system for a food delivery service, preprocessing the dataset is crucial to ensure that the features are properly \n",
    "prepared for modeling. Min-Max scaling is a technique used to normalize numerical features within a specific range, typically between 0 and 1. \n",
    "\n",
    "This helps to bring all the features to a similar scale, preventing features with larger values from dominating the modeling process. \n",
    "Here's how you would use Min-Max scaling to preprocess the dataset's features:\n",
    "\n",
    "Let's assume you have three features: price, rating, and delivery time.\n",
    "\n",
    "Understand the Features:\n",
    "    First, you should have a clear understanding of the features and their ranges. For example, price might range from $5 to $50, rating might \n",
    "    range from 1 to 5, and delivery time might range from 15 minutes to 60 minutes.\n",
    "\n",
    "Import Libraries:\n",
    "    You'll need to import the necessary libraries, such as Python's sklearn library, which provides various preprocessing tools, including Min-Max \n",
    "    scaling.\n",
    "\n",
    "Extract the Features:\n",
    "    Load your dataset and extract the relevant features you want to scale.\n",
    "\n",
    "Apply Min-Max Scaling:\n",
    "    For each feature, perform Min-Max scaling using the following formula:\n",
    "        \n",
    "--->>>     scaled_value = (x - min_value) / (max_value - min_value)\n",
    "\n",
    "where x is the original value of the feature, min_value is the minimum value of the feature in the dataset, and max_value is the maximum value of\n",
    "the feature in the dataset.\n",
    "Apply this formula to each value in your dataset for each feature.\n",
    "\n",
    "Implement Min-Max Scaling:\n",
    "    In Python, you can use the MinMaxScaler class from sklearn.preprocessing to easily apply Min-Max scaling. Here's how:\n",
    "    \n",
    "-->        from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "            # Create an instance of MinMaxScaler\n",
    "            scaler = MinMaxScaler()\n",
    "\n",
    "            # Fit the scaler on your data and transform the features\n",
    "-->        scaled_features = scaler.fit_transform(your_data)\n",
    "\n",
    "After applying this, scaled_features will contain the scaled values of your features.\n",
    "\n",
    "Interpretation:\n",
    "    The scaled values will now fall within the range of 0 to 1 for each feature. This ensures that all features are on the same scale and avoids\n",
    "    any feature having a disproportionate impact on the recommendation system.\n",
    "\n",
    "Model Building:\n",
    "    With the scaled features, you can proceed to build your recommendation system using appropriate modeling techniques, such as collaborative \n",
    "    filtering or content-based filtering, depending on the nature of your data and the requirements of your project.\n",
    "\n",
    "Remember that Min-Max scaling is just one of many preprocessing techniques, and its use should be guided by the nature of your data and the \n",
    "requirements of your recommendation system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddac62e2-5f60-496c-9bb3-c21680570598",
   "metadata": {},
   "source": [
    "## Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a11d77-271b-455f-9ccf-dc20880bd9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Principal Component Analysis (PCA) is a dimensionality reduction technique commonly used in machine learning to transform high-dimensional \n",
    "data into a lower-dimensional representation while preserving as much of the original variability as possible. In the context of building a model \n",
    "to predict stock prices using a dataset with numerous features, such as company financial data and market trends, PCA can help in reducing the \n",
    "complexity of the dataset and improving model performance. Here's how you would use PCA to achieve this:\n",
    "\n",
    "Understand the Dataset:\n",
    "    Before applying PCA, it's important to have a good understanding of your dataset's features and their relevance to predicting stock prices. \n",
    "    Features could include financial ratios, market indicators, historical stock prices, and so on.\n",
    "\n",
    "Standardize the Data:\n",
    "    PCA is sensitive to the scale of features, so it's important to standardize the data (mean = 0, standard deviation = 1) before applying PCA. \n",
    "    This ensures that all features are treated equally during the dimensionality reduction process.\n",
    "\n",
    "Calculate Covariance Matrix:\n",
    "    PCA works by finding the directions (principal components) along which the data varies the most. The first step is to calculate the covariance \n",
    "    matrix of the standardized data. The covariance matrix indicates how much two variables change together. It's important for understanding the \n",
    "    relationships between features.\n",
    "\n",
    "Calculate Eigenvectors and Eigenvalues:\n",
    "    From the covariance matrix, you can calculate the eigenvectors and eigenvalues. Eigenvectors represent the directions of maximum variance\n",
    "    in the data, and eigenvalues represent the amount of variance explained by each eigenvector.\n",
    "\n",
    "Sort Eigenvalues and Select Components:\n",
    "    Sort the eigenvalues in descending order. The eigenvectors corresponding to the largest eigenvalues are the principal components that capture\n",
    "    the most variance in the data. You can decide on the number of principal components to retain based on how much cumulative variance you want \n",
    "    to explain. A common approach is to choose components that explain a significant portion of the total variance, e.g., 95% or 99%.\n",
    "\n",
    "Project Data onto Principal Components:\n",
    "    Once you've selected the desired number of principal components, project your standardized data onto these components. This involves taking \n",
    "    a dot product between the standardized data and the selected eigenvectors.\n",
    "\n",
    "Transform the Data:\n",
    "    The result of the projection is a lower-dimensional representation of your data. This transformed data can be used as input for your \n",
    "    predictive model.\n",
    "\n",
    "Model Building:\n",
    "    You can now use the transformed data as input to your stock price prediction model. You may find that your model performs well with reduced \n",
    "    dimensionality because the principal components capture the most important variability in the data.\n",
    "\n",
    "It's important to note that while PCA can help reduce dimensionality and improve model efficiency, it might also lead to loss of interpretability \n",
    "since the principal components are linear combinations of the original features. Additionally, PCA might not always be the best choice if the \n",
    "relationships between features are complex and nonlinear.\n",
    "\n",
    "Experiment with different numbers of principal components and monitor the impact on model performance to find the right balance between \n",
    "dimensionality reduction and predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef7b020-4ddf-4010-b1d6-eb6676ebc749",
   "metadata": {},
   "source": [
    "## Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766bc851-d842-477b-bed4-4415b597e5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "To perform Min-Max scaling and transform the values in the dataset [1, 5, 10, 15, 20] to a range of -1 to 1, you can follow these steps:\n",
    "\n",
    "Compute Min and Max Values:\n",
    "Calculate the minimum and maximum values in the dataset.\n",
    "\n",
    "Minimum value: 1\n",
    "Maximum value: 20\n",
    "Apply Min-Max Scaling Formula:\n",
    "For each value in the dataset, apply the Min-Max scaling formula:\n",
    "    \n",
    "    scaled_value = -1 + (2 * (x - min_value) / (max_value - min_value))\n",
    "\n",
    "Where x is the original value, min_value is the minimum value (1), and max_value is the maximum value (20).\n",
    "\n",
    "Perform Min-Max Scaling:\n",
    "Apply the formula to each value in the dataset:\n",
    "\n",
    "For x = 1: scaled_value = -1 + (2 * (1 - 1) / (20 - 1)) = -1\n",
    "For x = 5: scaled_value = -1 + (2 * (5 - 1) / (20 - 1)) = -0.6\n",
    "For x = 10: scaled_value = -1 + (2 * (10 - 1) / (20 - 1)) = -0.2\n",
    "For x = 15: scaled_value = -1 + (2 * (15 - 1) / (20 - 1)) = 0.2\n",
    "For x = 20: scaled_value = -1 + (2 * (20 - 1) / (20 - 1)) = 0.6\n",
    "\n",
    "So, the Min-Max scaled values for the given dataset [1, 5, 10, 15, 20] in the range of -1 to 1 are approximately [-1, -0.6, -0.2, 0.2, 0.6]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690a14c3-9aa5-4c7d-bc2e-8e1ee250dd92",
   "metadata": {},
   "source": [
    "## Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b82f577-5b83-45de-8928-4e6c27b976e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "To determine the number of principal components to retain in a feature extraction using PCA, you typically aim to retain a sufficient number of \n",
    "components to explain a significant portion of the variance in the data. This decision often involves choosing a cumulative explained variance\n",
    "threshold, which reflects the amount of information you're willing to retain.\n",
    "\n",
    "Here's how you could approach determining the number of principal components to retain for your dataset containing features [height, weight, age,\n",
    "gender, blood pressure]:\n",
    "\n",
    "Standardize the Data:\n",
    "    Start by standardizing the data so that all features have mean 0 and standard deviation 1. PCA is sensitive to the scale of features, and \n",
    "    standardization ensures that all features are treated equally during the dimensionality reduction process.\n",
    "\n",
    "Calculate Covariance Matrix and Eigenvalues:\n",
    "    Calculate the covariance matrix of the standardized data and then compute the eigenvectors and eigenvalues. These eigenvalues represent the \n",
    "    amount of variance explained by each principal component.\n",
    "\n",
    "Sort Eigenvalues:\n",
    "    Sort the eigenvalues in descending order. The larger eigenvalues correspond to principal components that capture more variance in the data.\n",
    "\n",
    "Calculate Cumulative Explained Variance:\n",
    "    Calculate the cumulative explained variance by summing up the eigenvalues. Divide each eigenvalue by the total sum of eigenvalues to get the\n",
    "    proportion of variance explained by that component. Then, calculate the cumulative sum of these proportions.\n",
    "\n",
    "Choose the Number of Components:\n",
    "    Decide on a cumulative explained variance threshold that suits your needs. This threshold represents the minimum amount of variance you want \n",
    "    to retain in your reduced-dimensional representation. A common threshold might be 95% or 99%, indicating that you want to retain principal\n",
    "    components that collectively explain that much of the original data's variance.\n",
    "\n",
    "Retain Principal Components:\n",
    "    Retain the principal components that, when added up, cross your chosen cumulative explained variance threshold. These are the components that\n",
    "    capture the most important variability in your data.\n",
    "\n",
    "Project Data onto Chosen Components:\n",
    "Project your original standardized data onto the retained principal components to obtain the lower-dimensional representation.\n",
    "\n",
    "The exact number of principal components to retain will depend on the specific nature of your data and the amount of variability you're \n",
    "comfortable sacrificing for dimensionality reduction. A good approach is to plot the cumulative explained variance against the number of \n",
    "components and visually inspect the point at which the curve starts to level off. This point is often a reasonable choice for the number of \n",
    "components to retain.\n",
    "\n",
    "Keep in mind that while PCA can help reduce dimensionality, it might also lead to a loss of interpretability since the principal components are \n",
    "linear combinations of the original features. Additionally, consider the specific requirements of your analysis and modeling objectives when \n",
    "making this decision."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
