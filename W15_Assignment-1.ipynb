{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb1e76c9-fd13-4f24-a4e6-c28152136910",
   "metadata": {},
   "source": [
    "## Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a14758-e8d1-4717-881a-83a7f724d591",
   "metadata": {},
   "outputs": [],
   "source": [
    "Simple Linear Regression:\n",
    "    Simple linear regression is a statistical method used to model the relationship between two variables: a dependent variable\n",
    "    (also known as the response variable) and an independent variable (also known as the predictor variable). The goal of simple linear regression \n",
    "    is to find a linear relationship that best fits the data points. This linear relationship is represented by a straight line equation: \n",
    "        Y = a + bX, where Y is the dependent variable, X is the independent variable, \"a\" is the intercept, and \"b\" is the slope of the line.\n",
    "\n",
    "Example of Simple Linear Regression:\n",
    "Let's consider a scenario where we want to predict a student's final exam score (Y) based on the number of hours they studied (X). \n",
    "Here, the final exam score is the dependent variable, and the number of hours studied is the independent variable.\n",
    "\n",
    "Hours Studied (X)\tExam Score (Y)\n",
    "        2\t              65\n",
    "        3\t              75\n",
    "        4\t              80\n",
    "        5\t              85\n",
    "        \n",
    "Using simple linear regression, we can find the best-fitting line that represents the relationship between hours studied and exam score. \n",
    "The equation might look like: Exam Score = 60 + 5 * Hours Studied.\n",
    "\n",
    "Multiple Linear Regression:\n",
    "    Multiple linear regression extends the concept of simple linear regression to include more than one independent variable. Instead of modeling \n",
    "    the relationship between two variables, multiple linear regression models the relationship between a dependent variable and multiple \n",
    "    independent variables. The goal is to find a linear equation that accounts for the combined effects of these variables on the dependent \n",
    "    variable.\n",
    "\n",
    "Example of Multiple Linear Regression:\n",
    "Let's consider a scenario where we want to predict a car's fuel efficiency (Y) based on its engine size (X1) and weight (X2). Here, fuel efficiency\n",
    "is the dependent variable, while engine size and weight are the independent variables.\n",
    "\n",
    "Engine Size (X1)\tWeight (X2)\t Fuel Efficiency (Y)\n",
    "        2.0\t            3000\t        30\n",
    "        2.5\t            3200\t        28\n",
    "        3.0\t            3500\t        25\n",
    "        2.2\t            3100\t        29\n",
    "\n",
    "Using multiple linear regression, we can find the best-fitting plane that represents the relationship between engine size, weight, and fuel \n",
    "efficiency. The equation might look like: Fuel Efficiency = 35 - 2 * Engine Size + 0.01 * Weight.\n",
    "\n",
    "In summary, simple linear regression deals with the relationship between two variables, while multiple linear regression involves the relationship \n",
    "between a dependent variable and multiple independent variables. Both techniques aim to find linear equations that best describe these relationships\n",
    "based on observed data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc6653c-35dd-46c5-aee8-9c96aaf3efe4",
   "metadata": {},
   "source": [
    "## Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcb1a7f-9e6d-43fa-a162-eb9635268a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "Linear regression makes several assumptions about the data and the relationship between the variables. \n",
    "These assumptions are important because violating them can lead to unreliable results and incorrect conclusions. \n",
    "Here are the main assumptions of linear regression:\n",
    "\n",
    "Linearity: \n",
    "    The relationship between the independent and dependent variables should be linear. This means that the change in the dependent variable should\n",
    "    be proportional to a change in the independent variables.\n",
    "\n",
    "Independence: \n",
    "    The residuals (the differences between actual and predicted values) should be independent of each other. This assumption ensures that one \n",
    "    residual does not provide information about the others.\n",
    "\n",
    "Homoscedasticity: \n",
    "    The variance of the residuals should be constant across all levels of the independent variables. In simpler terms, the spread of the residuals\n",
    "    should be consistent.\n",
    "\n",
    "Normality: \n",
    "    The residuals should follow a normal distribution. This is important because many statistical techniques assume normally distributed data.\n",
    "\n",
    "No or Little Multicollinearity: \n",
    "    Multicollinearity occurs when independent variables are highly correlated with each other. This can make it difficult to separate the \n",
    "    individual effects of each independent variable on the dependent variable.\n",
    "\n",
    "To check whether these assumptions hold in a given dataset, you can use various techniques:\n",
    "\n",
    "Residual Plot: \n",
    "    Create a scatter plot of the residuals against the predicted values. If the plot shows a random pattern around zero without any clear trend, \n",
    "    it suggests that the linearity and homoscedasticity assumptions are met.\n",
    "\n",
    "Normality Plot (Q-Q Plot): \n",
    "    Create a quantile-quantile plot of the residuals. If the points closely follow the diagonal line, it suggests that the normality assumption is \n",
    "    met.\n",
    "\n",
    "Histogram and Normality Tests: \n",
    "    Examine the histogram of the residuals and perform normality tests (e.g., Shapiro-Wilk test, Anderson-Darling test). If the residuals \n",
    "    approximate a bell-shaped curve and the p-value from the normality test is not significantly low, the normality assumption is likely met.\n",
    "\n",
    "Variance Inflation Factor (VIF): \n",
    "    Calculate the VIF for each independent variable to assess multicollinearity. VIF values above a certain threshold (often 10) can indicate high \n",
    "    multicollinearity.\n",
    "\n",
    "Durbin-Watson Test: \n",
    "    This test checks for autocorrelation in the residuals. If the test statistic is close to 2, it suggests no autocorrelation.\n",
    "\n",
    "Cook's Distance: \n",
    "    Identify influential data points that might have a significant impact on the regression results. High Cook's distance values can indicate \n",
    "    potential issues.\n",
    "\n",
    "It is important to note that real-world data rarely perfectly adheres to all assumptions. However, the key is to assess whether violations of these \n",
    "assumptions are substantial enough to undermine the validity of the results. If assumptions are significantly violated, alternative regression\n",
    "methods or data transformations might be considered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2258bc58-07d1-496c-8ce7-2e83fbc4564f",
   "metadata": {},
   "source": [
    "## Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4373bf6b-13a6-4157-9cd1-d2fede269b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "In a linear regression model, the slope and intercept have specific interpretations that help us understand the relationship \n",
    "between the independent and dependent variables. Let's break down their interpretations using a real-world scenario:\n",
    "\n",
    "Scenario: Predicting House Prices\n",
    "    Suppose you are a real estate agent and you want to predict the selling price of houses based on their size (in square feet). \n",
    "    We collect data on house sizes and their corresponding selling prices to build a linear regression model.\n",
    "\n",
    "Linear Regression Model:\n",
    "Selling Price = Intercept + Slope * Size\n",
    "\n",
    "In this equation:\n",
    "\n",
    "Intercept: \n",
    "    It is the value of the dependent variable (selling price) when the independent variable (size) is zero. However, this interpretation might \n",
    "    not make sense in context. In this scenario, an intercept of zero doesn't have a practical meaning. It's often best to interpret the intercept \n",
    "    in the context of the data range.\n",
    "\n",
    "Slope: \n",
    "    It represents the change in the dependent variable (selling price) for a one-unit change in the independent variable (size). In other words, \n",
    "    it is the increase (or decrease) in selling price for every additional square foot of the house's size.\n",
    "\n",
    "Interpretation:\n",
    "Let's say we've found the following values for your linear regression model:\n",
    "Intercept = $50,000\n",
    "Slope = $100\n",
    "\n",
    "Intercept: \n",
    "    The intercept of $50,000 does not necessarily mean that a house with zero square feet has a selling price of $50,000. Rather, it indicates \n",
    "    that when a house has a size of zero (which is unrealistic), the predicted selling price would start at around $50,000. The intercept sets the \n",
    "    baseline value of the selling price.\n",
    "\n",
    "Slope: \n",
    "    The slope of $100 means that for every additional square foot in house size, the predicted selling price increases by $100. So, if you have a \n",
    "    house that is 500 square feet larger than another, you would expect the selling price of the larger house \n",
    "    to be $50,000 + ($100 * 500) = $100,000 higher.\n",
    "\n",
    "Keep in mind that these interpretations hold under the assumption that the linear regression model is appropriate and the underlying assumptions\n",
    "are met. In practice, we would want to evaluate the model's goodness of fit, check for assumptions, and consider other factors that could affect\n",
    "house prices, such as location, amenities, and market trends."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dd6f51-3a22-4228-9a5c-b72495a7216f",
   "metadata": {},
   "source": [
    "## Q4. Explain the concept of gradient descent. How is it used in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bee9cb-5815-4b1c-ae50-07ac4690b8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gradient Descent is an optimization algorithm used to minimize the loss function of a machine learning model. It's a key concept in training \n",
    "models, especially in the context of supervised learning. The goal of gradient descent is to iteratively adjust the model's parameters in a way \n",
    "that reduces the error or loss between the predicted values and the actual target values.\n",
    "\n",
    "Here's how gradient descent works:\n",
    "\n",
    "Initialization: \n",
    "    The process starts with initializing the model's parameters (weights and biases) with some initial values.\n",
    "\n",
    "Forward Pass: \n",
    "    The input data is fed through the model, and predictions are generated.\n",
    "\n",
    "Loss Calculation: \n",
    "    The difference between the predicted values and the actual target values is calculated using a loss function (also known as a cost function). \n",
    "    The loss function quantifies how far off the model's predictions are from the truth.\n",
    "\n",
    "Gradient Calculation: \n",
    "    The gradient of the loss function with respect to each parameter is calculated. The gradient represents the direction and magnitude of the \n",
    "    steepest increase in the loss. It tells us how much each parameter needs to be adjusted to reduce the loss.\n",
    "\n",
    "Parameter Update: \n",
    "    The parameters are adjusted in the opposite direction of the gradient to minimize the loss. The magnitude of the adjustment is determined by\n",
    "    the learning rate, which is a hyperparameter set by the user. A small learning rate can lead to slow convergence, while a large learning rate \n",
    "    can cause overshooting and divergence.\n",
    "\n",
    "Repeat: \n",
    "    Steps 2-5 are repeated iteratively for a certain number of epochs or until the loss reaches an acceptable level.\n",
    "\n",
    "The process of adjusting the model parameters according to the gradients continues until the loss function reaches a local minimum. \n",
    "This local minimum represents the best possible parameter values for the model given the training data.\n",
    "\n",
    "Gradient descent is used in machine learning to train various types of models, including linear regression, neural networks, and support vector \n",
    "machines, among others. It's especially powerful in training deep neural networks where the loss surface can be high-dimensional and non-convex.\n",
    "\n",
    "There are different variations of gradient descent, such as stochastic gradient descent (SGD), mini-batch gradient descent, and more advanced \n",
    "methods like Adam and RMSProp, which incorporate adaptive learning rates and momentum. These variations aim to improve the efficiency and \n",
    "convergence of the optimization process.\n",
    "\n",
    "Overall, gradient descent is a fundamental technique that plays a crucial role in enabling machine learning models to learn from data and make \n",
    "accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127cd5cc-5077-453b-b8ea-029df18b657e",
   "metadata": {},
   "source": [
    "## Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66873a4d-4873-4ca0-9004-d02e81463388",
   "metadata": {},
   "outputs": [],
   "source": [
    "Multiple linear regression is an extension of simple linear regression that allows for the modeling of relationships between a dependent variable \n",
    "and multiple independent variables. In simple linear regression, there's only one independent variable, whereas in multiple linear regression, \n",
    "there are two or more independent variables. The multiple linear regression model is represented by the following equation:\n",
    "\n",
    "Y = β0 + β1X1 + β2X2 + ... + βn*Xn + ε\n",
    "\n",
    "Where:\n",
    "\n",
    "Y is the dependent variable (the one you're trying to predict).\n",
    "β0 is the intercept.\n",
    "β1, β2, ..., βn are the coefficients of the independent variables X1, X2, ..., Xn, respectively.\n",
    "X1, X2, ..., Xn are the independent variables.\n",
    "ε is the error term or residual, representing the difference between the predicted and actual values.\n",
    "Differences between Multiple Linear Regression and Simple Linear Regression:\n",
    "\n",
    "Number of Independent Variables: \n",
    "    In simple linear regression, there's only one independent variable. In multiple linear regression, there are two or more independent variables\n",
    "    This allows the model to capture more complex relationships between the dependent variable and the predictors.\n",
    "\n",
    "Equation: \n",
    "    Simple linear regression has a straightforward equation: Y = β0 + β1*X + ε, where there's only one coefficient (β1) and one predictor (X). \n",
    "    In multiple linear regression, the equation includes multiple coefficients and predictors, resulting in a more complex equation.\n",
    "\n",
    "Model Complexity: \n",
    "    Multiple linear regression models are generally more complex than simple linear regression models. This increased complexity allows for better \n",
    "    fitting to data but also requires more careful interpretation and validation.\n",
    "\n",
    "Interpretation: \n",
    "    In simple linear regression, the slope coefficient (β1) represents the change in the dependent variable (Y) for a one-unit change in the \n",
    "    independent variable (X). In multiple linear regression, each slope coefficient represents the change in Y for a one-unit change in the \n",
    "    corresponding independent variable, while holding other variables constant. The intercept (β0) in both cases represents the value of Y when \n",
    "    all independent variables are zero, though this interpretation might not be meaningful in certain contexts.\n",
    "\n",
    "Assumptions: \n",
    "    The assumptions for multiple linear regression are similar to those for simple linear regression, but they extend to include multiple \n",
    "    independent variables. These assumptions include linearity, independence of residuals, homoscedasticity, normality of residuals, and no \n",
    "    multicollinearity.\n",
    "\n",
    "Dimensionality: \n",
    "    Multiple linear regression deals with higher-dimensional data and requires careful consideration of multicollinearity issues. \n",
    "    Multicollinearity occurs when independent variables are highly correlated, which can make it difficult to interpret the impact of each variable\n",
    "    individually.\n",
    "\n",
    "In summary, multiple linear regression is a more versatile model that can handle situations where there are multiple predictors influencing the \n",
    "dependent variable. It allows for more nuanced and realistic modeling of complex relationships in real-world data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95206980-a2cb-42cc-a677-702d12d17c5c",
   "metadata": {},
   "source": [
    "## Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c240b2f-3c0f-4392-ae8f-1dcedcf21cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Multicollinearity is a phenomenon in multiple linear regression where two or more independent variables in the model are highly correlated with\n",
    "each other. This can cause problems because it becomes difficult to distinguish the individual effects of these correlated variables on the \n",
    "dependent variable. In other words, multicollinearity can lead to instability in the model's coefficients, making their interpretations less r\n",
    "eliable.\n",
    "\n",
    "Detecting Multicollinearity:\n",
    "\n",
    "    Correlation Matrix: \n",
    "        Calculate the correlation coefficients between all pairs of independent variables. If there are variables with high absolute correlation \n",
    "        coefficients (close to 1 or -1), it indicates potential multicollinearity.\n",
    "\n",
    "    Variance Inflation Factor (VIF): \n",
    "        VIF measures how much the variance of the estimated regression coefficient is increased due to multicollinearity. High VIF values (usually \n",
    "        above 10) indicate the presence of multicollinearity.\n",
    "\n",
    "    Tolerance: \n",
    "        Tolerance is the reciprocal of VIF. Low tolerance values indicate high multicollinearity. Tolerance values less than 0.1 can be a \n",
    "        sign of a problem.\n",
    "\n",
    "Addressing Multicollinearity:\n",
    "\n",
    "    Remove or Combine Variables: \n",
    "        If two variables are highly correlated, consider removing one of them from the model. Alternatively, you can create a new variable that \n",
    "        combines the information from the correlated variables.\n",
    "\n",
    "    Feature Selection: \n",
    "        Use feature selection techniques to choose the most important variables for the model. This can help reduce the impact of multicollinearity\n",
    "        by focusing on the most relevant predictors.\n",
    "\n",
    "    Regularization: \n",
    "        Techniques like Ridge Regression and Lasso Regression add penalty terms to the loss function, which can help mitigate multicollinearity by\n",
    "        shrinking the coefficients of correlated variables.\n",
    "\n",
    "    Principal Component Analysis (PCA): \n",
    "        PCA is a dimensionality reduction technique that can transform the original variables into a new set of orthogonal variables (principal \n",
    "        components) that are uncorrelated. This can help mitigate multicollinearity issues.\n",
    "\n",
    "    Collect More Data: \n",
    "        If possible, collecting more data can help mitigate multicollinearity. With more data points, the correlation between variables might \n",
    "        become less pronounced.\n",
    "\n",
    "    Domain Knowledge: \n",
    "        Use your understanding of the domain to determine whether the correlation between variables is expected or not. Some variables might \n",
    "        naturally be correlated due to the nature of the problem.\n",
    "\n",
    "Addressing multicollinearity is crucial to ensure that the regression model provides accurate and meaningful insights. High multicollinearity can\n",
    "lead to unstable coefficient estimates and make it difficult to interpret the effects of individual variables on the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ea4753-159c-46ba-8ac7-f6bc5ea98bc1",
   "metadata": {},
   "source": [
    "## Q7. Describe the polynomial regression model. How is it different from linear regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b416a01-5a43-4709-9700-7661b4abf010",
   "metadata": {},
   "outputs": [],
   "source": [
    "Polynomial regression is a type of regression analysis that allows for modeling relationships between the dependent variable and independent \n",
    "variables as polynomial functions. While linear regression models assume a linear relationship between variables, polynomial regression extends \n",
    "this concept to capture more complex and nonlinear relationships. In polynomial regression, higher-degree polynomial terms (quadratic, cubic, etc.)\n",
    "are included in the model equation to better fit the data.\n",
    "\n",
    "The general form of a polynomial regression model is:\n",
    "\n",
    "Y = β0 + β1X + β2X^2 + ... + βn*X^n + ε\n",
    "\n",
    "Where:\n",
    "\n",
    "Y is the dependent variable.\n",
    "β0, β1, β2, ..., βn are the coefficients of the polynomial terms.\n",
    "X is the independent variable.\n",
    "n is the degree of the polynomial.\n",
    "\n",
    "Differences between Polynomial Regression and Linear Regression:\n",
    "\n",
    "Equation Form: \n",
    "    In linear regression, the equation is a simple linear combination of the independent variables with their respective coefficients. \n",
    "    In polynomial regression, the equation includes higher-degree terms (quadratic, cubic, etc.) of the independent variable.\n",
    "\n",
    "Complexity: \n",
    "    Polynomial regression allows for modeling more complex and nonlinear relationships between variables. This is particularly useful when the true\n",
    "    relationship is not well represented by a straight line.\n",
    "\n",
    "Curve Fitting: \n",
    "    While linear regression fits a straight line to the data points, polynomial regression fits a curve that can better capture intricate patterns\n",
    "    and variations in the data.\n",
    "\n",
    "Overfitting Risk: \n",
    "    Polynomial regression can be prone to overfitting, especially when the degree of the polynomial is high. Overfitting occurs when the model fits \n",
    "    the training data too closely and fails to generalize well to new, unseen data.\n",
    "\n",
    "Model Interpretation: \n",
    "    In linear regression, the interpretation of coefficients is relatively straightforward. In polynomial regression, the interpretation becomes \n",
    "    more complex due to the presence of higher-degree terms. It's not always intuitive to explain the impact of a change in an independent variable\n",
    "    on the dependent variable in terms of higher-degree polynomial terms.\n",
    "\n",
    "Model Complexity: \n",
    "    Polynomial regression models with higher degrees can become very complex and computationally intensive, making them less practical for large \n",
    "    datasets.\n",
    "\n",
    "Polynomial regression is a valuable tool when the relationship between variables is nonlinear and can't be adequately represented by a simple \n",
    "straight line. However, it's important to use polynomial regression judiciously and avoid overfitting, which can lead to poor generalization \n",
    "performance on new data. Regularization techniques and cross-validation can help manage overfitting in polynomial regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c824c7a-0c4c-4540-a727-1ca0910c1694",
   "metadata": {},
   "source": [
    "## Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e87f5ab-d88c-4299-b8e8-a90f8b59ea03",
   "metadata": {},
   "outputs": [],
   "source": [
    "Advantages of Polynomial Regression:\n",
    "\n",
    "Flexibility in Modeling: \n",
    "    Polynomial regression can capture more complex and nonlinear relationships between variables, which linear regression might not be able to \n",
    "    represent accurately.\n",
    "\n",
    "Better Fit to Data: \n",
    "    When the true relationship between variables is curved or has intricate patterns, polynomial regression can provide a better fit to the data\n",
    "    by using higher-degree polynomial terms.\n",
    "\n",
    "Higher Degree of Accuracy: \n",
    "    For certain datasets, polynomial regression can result in higher accuracy compared to linear regression, especially when the underlying \n",
    "    relationship is nonlinear.\n",
    "\n",
    "Useful for Trend Analysis: \n",
    "    Polynomial regression is commonly used in fields like economics, social sciences, and environmental sciences to analyze trends and patterns \n",
    "    in historical data.\n",
    "\n",
    "\n",
    "Disadvantages of Polynomial Regression:\n",
    "\n",
    "Overfitting Risk: \n",
    "    Polynomial regression, especially with high-degree polynomials, is susceptible to overfitting. Overfitting occurs when the model fits the \n",
    "    training data too closely and performs poorly on new, unseen data.\n",
    "\n",
    "Complex Interpretation: \n",
    "    Higher-degree polynomial terms make the interpretation of coefficients more complex and less intuitive. Explaining the impact of a change in \n",
    "    an independent variable can become challenging.\n",
    "\n",
    "Computational Complexity: \n",
    "    As the degree of the polynomial increases, the complexity of the model and the computations required also increase. This can be a concern for \n",
    "    large datasets.\n",
    "\n",
    "Extrapolation Issues: \n",
    "    Polynomial regression models might produce unrealistic predictions when extrapolating beyond the range of the observed data, especially for \n",
    "    high-degree polynomials.\n",
    "\n",
    "\n",
    "When to Prefer Polynomial Regression:\n",
    "\n",
    "You might prefer to use polynomial regression in the following situations:\n",
    "\n",
    "Curved Relationships: \n",
    "    When the relationship between variables is clearly nonlinear, polynomial regression can provide a more accurate representation.\n",
    "\n",
    "Small Datasets: \n",
    "    If you have a relatively small dataset where a linear model doesn't capture the pattern well, polynomial regression might be a suitable \n",
    "    alternative.\n",
    "\n",
    "Exploratory Analysis: \n",
    "    Polynomial regression can be useful for exploratory analysis to understand the underlying patterns in the data before considering more complex\n",
    "    models.\n",
    "\n",
    "Domain Knowledge: \n",
    "    If you have domain knowledge that suggests a specific nonlinear relationship between variables, polynomial regression can be a good choice.\n",
    "\n",
    "Balance between Bias and Variance: \n",
    "    If you're aware of the risk of overfitting and take steps to manage it (e.g., regularization techniques), polynomial regression can provide a\n",
    "    good balance between bias and variance.\n",
    "\n",
    "In summary, polynomial regression offers greater flexibility in capturing nonlinear relationships, but it comes with the trade-off of increased \n",
    "complexity and potential overfitting. It's essential to carefully consider the dataset, the underlying relationships, and the risk of overfitting \n",
    "before deciding to use polynomial regression over linear regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
