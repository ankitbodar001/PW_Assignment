{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOPIC: Understanding Pooling and Padding in CNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Describe the purpose and benefits of pooling in CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Pooling, in Convolutional Neural Networks (CNNs), serves the purpose of reducing the spatial dimensions of the input volume, \n",
    "leading to a smaller representation while retaining important information. There are several types of pooling, with max pooling and average \n",
    "pooling being the most common.\n",
    "\n",
    "Purpose:\n",
    "    Dimensionality Reduction: \n",
    "        By downsampling the input feature maps, pooling reduces the number of parameters and computations in the network. This simplification helps\n",
    "        in controlling overfitting and computational costs.\n",
    "\n",
    "    Translation Invariance: \n",
    "        Pooling helps in achieving a degree of translation invariance. It means that even if the position of a feature in the input changes \n",
    "        slightly, the network can still recognize it due to pooling summarizing the presence of features in a local neighborhood.\n",
    "\n",
    "Benefits:\n",
    "    Computational Efficiency: \n",
    "        Pooling reduces the computational load by decreasing the spatial dimensions, making subsequent operations faster and more manageable.\n",
    "\n",
    "    Feature Generalization: \n",
    "        It extracts the most important features while discarding less relevant ones, aiding the network in focusing on significant patterns.\n",
    "\n",
    "    Robustness to Variations: \n",
    "        Pooling helps in making the network less sensitive to small variations in the input, enhancing its ability to recognize features \n",
    "        regardless of their exact location.\n",
    "\n",
    "    Memory Efficiency: \n",
    "        Smaller feature maps post-pooling reduce memory requirements, facilitating easier storage and manipulation of data.\n",
    "\n",
    "However, it's worth noting that with the introduction of newer architectures, like the inception modules and residual connections in networks \n",
    "like InceptionNet and ResNet, respectively, the use of pooling layers has become less frequent in some models. These architectures leverage \n",
    "different strategies to achieve similar goals while reducing spatial dimensions but with potentially less loss of information compared to \n",
    "traditional pooling layers.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explain the difference between min pooling and max pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''Min pooling and max pooling are both types of pooling operations used in Convolutional Neural Networks (CNNs) to downsample feature maps, \n",
    "reducing their spatial dimensions. However, they operate differently in terms of how they aggregate information within the pooling window.\n",
    "\n",
    "Max Pooling:\n",
    "    Operation: \n",
    "        Max pooling takes the maximum value from each sub-region of the input feature map within a defined window (often 2x2 or 3x3) and outputs \n",
    "        only the maximum value.\n",
    "\n",
    "    Purpose: \n",
    "        It captures the most active feature within the window, emphasizing the presence of specific features. Max pooling is effective in \n",
    "        preserving prominent features, enhancing robustness to translations and variations in the input data.\n",
    "\n",
    "    Example: \n",
    "        Given a 2x2 pooling window, if the values within that window are [3, 5, 2, 4], max pooling would output the value 5 (the maximum value\n",
    "        in the window).\n",
    "\n",
    "Min Pooling:\n",
    "    Operation: \n",
    "        Min pooling, on the other hand, takes the minimum value from each sub-region of the input feature map within the defined window and \n",
    "        outputs only the minimum value.\n",
    "\n",
    "    Purpose: \n",
    "        It focuses on capturing the least active feature within the window, highlighting the absence or minimum presence of specific features.\n",
    "\n",
    "    Example: \n",
    "        Using the same 2x2 pooling window as before with values [3, 5, 2, 4], min pooling would output the value 2 (the minimum value in the \n",
    "        window).\n",
    "\n",
    "Differences:\n",
    "    Aggregation: \n",
    "        Max pooling captures the most prominent features, while min pooling focuses on the least active or minimum features within the pooling \n",
    "        window.\n",
    " \n",
    "    Use Cases: \n",
    "        Max pooling is commonly used in CNN architectures due to its ability to capture strong, salient features, whereas min pooling is less \n",
    "        frequently used and might be more suitable for specific applications where the absence of certain features is crucial.\n",
    "\n",
    "While max pooling is more prevalent in CNN architectures due to its effectiveness in capturing important features, the choice between max and min\n",
    "pooling (or using average pooling) often depends on the specific requirements and characteristics of the task at hand.'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Discuss the concept of padding in CNN and its significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Padding in Convolutional Neural Networks (CNNs) refers to the technique of adding extra bordering pixels around the input data before applying\n",
    "convolution operations. This additional border of pixels is typically filled with zeros (zero-padding), although other padding methods like\n",
    "reflection padding or replication padding can also be used.\n",
    "\n",
    "Significance of Padding:\n",
    "    Preservation of Spatial Information:\n",
    "        Prevents Information Loss: Without padding, the spatial dimensions of the input feature maps reduce with each convolutional layer. \n",
    "        Padding helps retain the spatial dimensions, especially at deeper layers, preserving more spatial information.\n",
    "    Control over Output Size:\n",
    "        Desired Output Dimensions: Padding allows control over the spatial dimensions of the output feature maps after convolution. By adjusting\n",
    "        the amount of padding, it's possible to obtain specific output dimensions.\n",
    "    Mitigating Border Effects:\n",
    "        Reducing Border Effects: Convolution operations at the edges of feature maps tend to have fewer neighbors, causing border effects and \n",
    "        reduction in information. Padding helps mitigate this by allowing more convolutions at the edges, enhancing the network's performance and\n",
    "        reducing artifacts.\n",
    "    Facilitating Feature Learning:\n",
    "        Enabling Convolutional Operations: Padding ensures that features at the borders of the input are fully considered during convolution, \n",
    "        enabling the network to learn features at different positions effectively.\n",
    "    Compatibility with Stride and Kernel Sizes:\n",
    "        Flexible Use of Stride and Kernel: Padding enables the use of larger kernel sizes or strides without drastically reducing the spatial \n",
    "        dimensions of the feature maps. This flexibility in architectural choices aids in learning hierarchical representations of data.\n",
    "    \n",
    "Types of Padding:\n",
    "    Valid (No Padding): No padding is added, and convolution is performed only where the input and the kernel fully overlap, leading to reduction\n",
    "    in spatial dimensions.\n",
    "\n",
    "    Same Padding: It adds enough padding to the input so that the output feature map has the same spatial dimensions as the input. This is \n",
    "    achieved by adding padding such that the convolutional operation covers the input fully.\n",
    "\n",
    "Padding is a crucial concept in CNNs, as it impacts the network's ability to learn features effectively, control output dimensions, and mitigate \n",
    "issues related to border effects, ultimately contributing to the network's performance and robustness.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare and contrast zero-padding and valid-padding in terms of their effects on the output feature Map size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''Zero-padding and valid-padding are two common techniques used in Convolutional Neural Networks (CNNs) that have contrasting effects on the output\n",
    "feature map size.\n",
    "\n",
    "Zero-padding:\n",
    "    Effect on Output Size:\n",
    "        Preserves Output Size: Zero-padding adds extra rows and columns of zeros around the input feature map.\n",
    "        Maintains Spatial Dimensions: With zero-padding, the spatial dimensions of the output feature map can remain the same as the input when \n",
    "        using convolutional operations.\n",
    "    \n",
    "    Example:\n",
    "        Consider an input feature map of size 5x5 and a convolutional kernel of size 3x3. Applying zero-padding of size 1 (one pixel wide on each\n",
    "        side) would result in a padded input size of 7x7.\n",
    "        Convolution with a 3x3 kernel on this padded input would yield a 5x5 output feature map, maintaining the spatial dimensions of the input.\n",
    "\n",
    "Valid-padding:\n",
    "    Effect on Output Size:\n",
    "        Reduces Output Size: Valid-padding (also called no padding) does not add any extra bordering pixels to the input.\n",
    "        Decreases Spatial Dimensions: Without any padding, the spatial dimensions of the output feature map reduce compared to the input.\n",
    "\n",
    "    Example:\n",
    "        For the same example of a 5x5 input feature map and a 3x3 kernel, if valid-padding is used (no padding), the convolution operation would \n",
    "        be performed without adding any extra padding.\n",
    "        Applying the 3x3 kernel on the 5x5 input would result in a 3x3 output feature map due to the reduction in spatial dimensions caused by the\n",
    "        absence of padding.\n",
    "\n",
    "Comparison:\n",
    "    Output Size Maintenance: \n",
    "        Zero-padding maintains the spatial dimensions of the input in the output, while valid-padding reduces the output size by performing \n",
    "        convolution without any additional padding.\n",
    "\n",
    "    Control over Output Size: \n",
    "        Zero-padding allows for controlling the output size by adjusting the amount of padding added, while valid-padding results in a reduced \n",
    "        output size compared to the input.\n",
    "\n",
    "In summary, zero-padding is often used when preservation of spatial dimensions in the output feature maps is desired, while valid-padding is used \n",
    "when reducing spatial dimensions is acceptable or desired for downsampling or extracting key features.'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.  2.5 4. ]\n",
      "[2.5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the input feature map\n",
    "input_feature_map = [1, 2, 3]\n",
    "\n",
    "# Define the filter\n",
    "filters = [0, 1, 0.5]\n",
    "\n",
    "# Apply convolution with zero-padding\n",
    "output_feature_map_zero_padding = np.convolve(input_feature_map, filters, mode='same')\n",
    "\n",
    "# Print the output feature map size\n",
    "print(output_feature_map_zero_padding)\n",
    "\n",
    "# Apply convolution with valid-padding\n",
    "output_feature_map_valid_padding = np.convolve( input_feature_map, filters, mode='valid')\n",
    "\n",
    "# Print the output feature map size\n",
    "print(output_feature_map_valid_padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOPIC: Exploring LeNetÂ¶\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Provide a brief overview of LeNet-5 architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''LeNet-5 is a pioneering Convolutional Neural Network (CNN) architecture developed by Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner\n",
    "in the 1990s. It was one of the earliest successful CNN models and played a significant role in advancing the field of deep learning and computer\n",
    "vision. LeNet-5 was primarily designed for handwritten digit recognition tasks, such as recognizing digits in checks and postal services.\n",
    "\n",
    "Structure:\n",
    "\n",
    "    Input: 32x32 grayscale image\n",
    "    Layers:\n",
    "        3 convolutional layers: Each with 5x5 filters and ReLU activation\n",
    "        2 subsampling layers: Each using 2x2 average pooling with stride 2\n",
    "        1 flattening layer: Converts the feature maps to a vector\n",
    "        2 fully-connected layers: 120 and 84 neurons with ReLU activation\n",
    "        Output: 10 neurons with softmax activation for digit classification\n",
    "\n",
    "Key Points:\n",
    "\n",
    "    Small and efficient: Suitable for limited computational resources compared to modern CNNs\n",
    "    Emphasis on local features: Achieved through small receptive fields of 5x5 filters\n",
    "    Adaptive feature extraction: Utilized subsampling to reduce dimensionality and capture global features\n",
    "    Fully-connected layers: Performed final classification based on extracted features\n",
    "\n",
    "Significance:\n",
    "\n",
    "    Demonstrated the effectiveness of CNNs for image recognition tasks\n",
    "    Paved the way for the development of more complex and powerful CNN architectures\n",
    "    Remains a valuable tool for understanding the basic principles of CNNs\n",
    "\n",
    "Applications:\n",
    "\n",
    "    Handwritten digit recognition\n",
    "    Image classification\n",
    "    Feature extraction\n",
    "\n",
    "Limitations:\n",
    "\n",
    "    Limited to small input sizes\n",
    "    Less powerful than modern CNNs for complex tasks\n",
    "\n",
    "Overall, LeNet-5 represents a significant milestone in the history of deep learning and laid the groundwork for the widespread adoption of CNNs \n",
    "in various applications.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Describe the key components of LeNet-5 and their respective purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Key Components of LeNet-5 and their Purposes:\n",
    "\n",
    "1. Input Layer:\n",
    "\n",
    "    Purpose: This layer accepts the input image, typically a 32x32 grayscale image representing a handwritten digit.\n",
    "    Function: It preprocesses the input image and prepares it for subsequent processing by the network.\n",
    "\n",
    "2. Convolutional Layers:\n",
    "\n",
    "    Number: LeNet-5 has 3 convolutional layers.\n",
    "    Purpose: Extract local features from the input image.\n",
    "    Function: Each layer applies a set of filters (5x5 in this case) that slide across the image, extracting features like edges, corners, and \n",
    "    textures.\n",
    "    Activation: LeNet-5 uses the ReLU activation function in these layers, which introduces non-linearity and helps the network learn complex \n",
    "    features.\n",
    "\n",
    "3. Subsampling Layers:\n",
    "\n",
    "    Number: LeNet-5 has 2 subsampling layers.\n",
    "    Purpose: Reduce the dimensionality of the feature maps and capture global features.\n",
    "    Function: These layers use 2x2 average pooling with stride 2, which reduces the feature map size by half while preserving important information.\n",
    "\n",
    "4. Flattening Layer:\n",
    "\n",
    "    Purpose: Convert the feature maps into a single vector\n",
    "    Function: This layer transforms the multi-dimensional feature maps (e.g., 14x14x20) into a one-dimensional vector (e.g., 1960) suitable for\n",
    "    input to fully-connected layers.\n",
    "\n",
    "5. Fully-Connected Layers:\n",
    "\n",
    "    Number: LeNet-5 has two fully-connected layers with 120 and 84 neurons, respectively.\n",
    "    Purpose: Combine features extracted by convolutional layers and make final classification decision.\n",
    "    Function: These layers perform linear transformations on the input vector, allowing the network to learn complex relationships between \n",
    "    features and the output classes (10 digits in this case).\n",
    "    Activation: LeNet-5 uses ReLU activation in the first fully-connected layer and no activation in the second (softmax activation is applied\n",
    "    in the output layer).\n",
    "\n",
    "6. Output Layer:\n",
    "\n",
    "    Number: LeNet-5 has a single output layer with 10 neurons.\n",
    "    Purpose: Predict the class label of the input image.\n",
    "    Function: Each neuron in the output layer represents a digit (0-9). The network calculates the probability that the input image belongs to \n",
    "    each class and outputs the class with the highest probability.\n",
    "    Activation: LeNet-5 uses the softmax activation function in this layer, which normalizes the outputs into probabilities between 0 and 1.\n",
    "\n",
    "Summary:\n",
    "\n",
    "    Each component of LeNet-5 plays a crucial role in the overall process of recognizing handwritten digits. The convolutional layers extract \n",
    "    local features, subsampling layers capture global information, fully-connected layers combine features and learn complex relationships, and \n",
    "    finally, the output layer makes the final prediction. This architecture highlights the fundamental principles of CNNs and demonstrates their \n",
    "    effectiveness in image recognition tasks.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Discuss the advantages and limitations of LeNet-5 in the context of image classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Advantages of LeNet-5:\n",
    "    \n",
    "    Simplicity and efficiency: \n",
    "        LeNet-5 has a relatively small number of parameters and layers, making it computationally efficient and suitable for limited resources \n",
    "        compared to modern CNNs.\n",
    "    Focus on local features: \n",
    "        The use of small receptive fields in the convolutional layers allows LeNet-5 to effectively capture local features in images, which are \n",
    "        crucial for tasks like handwritten digit recognition.\n",
    "    Adaptive feature extraction: \n",
    "        Subsampling layers downsample the feature maps while preserving important information, allowing LeNet-5 to capture both local and global \n",
    "        features.\n",
    "    Demonstrated effectiveness: \n",
    "        LeNet-5 achieved state-of-the-art results on handwritten digit recognition tasks, paving the way for the development of more complex CNNs.\n",
    "    Educational value: \n",
    "        Due to its simplicity, LeNet-5 remains a valuable tool for understanding the basic principles of CNNs and serves as a foundational model \n",
    "        for further research.\n",
    "    Potential for transfer learning: \n",
    "        The pre-trained features extracted by LeNet-5 can be used as input to other models, improving their performance on similar tasks.\n",
    "\n",
    "Limitations of LeNet-5:\n",
    "\n",
    "    Limited to small input sizes:\n",
    "        LeNet-5 is only suitable for processing small images (e.g., 32x32) and cannot handle high-resolution images without significant \n",
    "        modifications.\n",
    "    Less powerful than modern CNNs: \n",
    "        Due to its limited complexity, LeNet-5 performs poorly on complex image classification tasks compared to modern architectures like VGG or\n",
    "        ResNet.\n",
    "    Overfitting risk: \n",
    "        The relatively small number of parameters in LeNet-5 can lead to overfitting, especially when trained on small datasets.\n",
    "    Limited representational capacity: \n",
    "        The shallow architecture of LeNet-5 restricts its ability to learn complex relationships between features, which can hinder performance on\n",
    "        tasks with high intra-class variability.\n",
    "    Not suitable for object detection or segmentation: \n",
    "        LeNet-5 was designed for digit recognition and cannot be directly applied to tasks like object detection or segmentation without \n",
    "        significant modifications.\n",
    "\n",
    "Conclusion:\n",
    "    Despite its limitations, LeNet-5 remains a significant milestone in the history of CNNs. Its simplicity, efficiency, and effectiveness in \n",
    "    specific tasks make it a valuable tool for learning and understanding the fundamentals of these powerful models. However, it is important to \n",
    "    acknowledge that LeNet-5 is no longer competitive for most modern image classification tasks. More complex and advanced architectures have \n",
    "    emerged that offer significantly improved performance on a wider range of applications.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implement LeNet-5 using a deep learning framework of youc choice (e.g., TensorFlow, PyTorch) and train it on a publicly available dataset (e.g., MNIST). Evaluate its performance and provide insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images = train_images.reshape(-1, 28, 28, 1)\n",
    "test_images = test_images.reshape(-1, 28, 28, 1)\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Conv2D(filters=6, kernel_size=(5, 5), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.AveragePooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(filters=16, kernel_size=(5, 5), activation='relu'),\n",
    "    layers.AveragePooling2D(pool_size=(2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(120, activation='relu'),\n",
    "    layers.Dense(84, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2329 - accuracy: 0.9291 - val_loss: 0.1074 - val_accuracy: 0.9670\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0839 - accuracy: 0.9741 - val_loss: 0.0611 - val_accuracy: 0.9813\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0600 - accuracy: 0.9814 - val_loss: 0.0592 - val_accuracy: 0.9806\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0464 - accuracy: 0.9855 - val_loss: 0.0490 - val_accuracy: 0.9834\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0379 - accuracy: 0.9878 - val_loss: 0.0503 - val_accuracy: 0.9839\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0312 - accuracy: 0.9899 - val_loss: 0.0450 - val_accuracy: 0.9859\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 7s 3ms/step - loss: 0.0281 - accuracy: 0.9908 - val_loss: 0.0282 - val_accuracy: 0.9906\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0221 - accuracy: 0.9926 - val_loss: 0.0418 - val_accuracy: 0.9874\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 7s 3ms/step - loss: 0.0202 - accuracy: 0.9933 - val_loss: 0.0347 - val_accuracy: 0.9892\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0188 - accuracy: 0.9940 - val_loss: 0.0343 - val_accuracy: 0.9889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x294afee90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(train_images, train_labels, epochs=10, validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.0343 - accuracy: 0.9889\n",
      "Test loss: 0.03432998061180115\n",
      "Test accuracy: 0.9889000058174133\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print('Test loss:', test_loss)\n",
    "print('Test accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Insights:\n",
    "\n",
    "This implementation achieves a test accuracy of approximately 99%. This demonstrates the effectiveness of LeNet-5 for simple image classification \n",
    "tasks like MNIST digit recognition. However, it's important to note that this performance is expected and not particularly remarkable compared to\n",
    "modern CNNs, which can often achieve 99.7% or higher accuracy on MNIST.\n",
    "\n",
    "Here are some additional insights:\n",
    "\n",
    "    LeNet-5's simplicity makes it ideal for educational purposes and understanding CNN fundamentals.\n",
    "    Its computational efficiency allows it to be deployed on limited resources.\n",
    "    However, its limitations in representing complex relationships and handling high-resolution images restrict its applicability to many modern \n",
    "    tasks.\n",
    "\n",
    "Overall, LeNet-5 serves as a valuable historical reference and foundational model for CNNs. While not the most powerful architecture today, it \n",
    "offers valuable lessons and provides a basis for understanding more complex and effective models.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOPIC: Analyzing AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Present an overview of the AlexNet architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''AlexNet Architecture Overview:\n",
    "A Milestone in CNN History:\n",
    "    AlexNet, developed by Alex Krizhevsky et al. in 2012, was a groundbreaking convolutional neural network (CNN) that revolutionized the field of \n",
    "    image recognition. Its introduction at the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012 marked a significant performance \n",
    "    jump, winning the competition by a large margin and sparking a resurgence of interest in deep learning.\n",
    "\n",
    "Key Features:\n",
    "\n",
    "    Increased depth: \n",
    "        AlexNet boasted a significantly deeper architecture compared to previous CNNs, with 8 layers including 5 convolutional layers and 3 \n",
    "        fully-connected layers. This increased depth allowed the network to learn more complex relationships between features.\n",
    "    Rectified Linear Unit (ReLU) activation: \n",
    "        AlexNet introduced the use of ReLU activation instead of the traditional tanh or sigmoid functions, improving training speed and \n",
    "        performance.\n",
    "    Max pooling: \n",
    "        Max pooling layers were used to downsample feature maps, reducing computational cost and improving feature generalization.\n",
    "    Dropout layers: \n",
    "        Dropout layers randomly set neuron outputs to zero during training, preventing overfitting and improving generalization.\n",
    "    Larger image size: \n",
    "        AlexNet used larger input images (227x227) compared to previous models, allowing it to capture more context and detail.\n",
    "    Data augmentation: \n",
    "        AlexNet employed data augmentation techniques like random cropping and flipping to artificially increase the training data size, improving\n",
    "        robustness.\n",
    "    GPU utilization: \n",
    "        AlexNet was one of the first CNNs to utilize GPUs for training, significantly accelerating the process and making large-scale training \n",
    "        feasible.\n",
    "\n",
    "Impact:\n",
    "\n",
    "    Performance boost: \n",
    "        AlexNet's performance on the ILSVRC competition surpassed previous methods by a significant margin, demonstrating the potential of deep \n",
    "        learning for image recognition.\n",
    "    Increased research: \n",
    "        AlexNet's success sparked a surge of interest in deep learning research, leading to the development of numerous advanced CNN architectures\n",
    "        and applications.\n",
    "    Foundation for future models: \n",
    "        AlexNet's key features and design principles served as a foundation for many subsequent successful CNN architectures.\n",
    "\n",
    "Limitations:\n",
    "\n",
    "    Computational cost: \n",
    "        Training AlexNet required significant computational resources compared to previous models, limiting its accessibility to some researchers.\n",
    "    Overfitting risk:\n",
    "        Despite utilizing dropout, AlexNet could still suffer from overfitting with limited training data.\n",
    "    Limited task applicability: \n",
    "        While excelling at image classification, AlexNet's architecture was not directly applicable to other tasks like object detection or \n",
    "        segmentation.\n",
    "\n",
    "Overall:\n",
    "\n",
    "    Despite its limitations, AlexNet remains a landmark achievement in the history of computer vision and deep learning. Its innovative \n",
    "    architecture and impressive performance paved the way for the development of more powerful and versatile CNNs, shaping the modern landscape \n",
    "    of image recognition and artificial intelligence.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explain the architectural innovations introduced in AlexNet that contributed to its breakthrough performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''AlexNet's breakthrough performance on the ILSVRC competition can be attributed to several key architectural innovations:\n",
    "\n",
    "Increased Depth:\n",
    "    Compared to previous CNNs with just a few layers, AlexNet used 8 layers, including 5 convolutional layers and 3 fully-connected layers. \n",
    "    This significant increase in depth allowed the network to learn more complex and hierarchical representations of visual features, leading to\n",
    "    improved image recognition accuracy.\n",
    "\n",
    "Rectified Linear Unit (ReLU) Activation:\n",
    "    AlexNet replaced the traditional tanh or sigmoid activations with the ReLU function. ReLU has a faster and more efficient computation, \n",
    "    allowing for faster training and improved performance. Additionally, ReLU's non-saturating nature helps prevent the vanishing gradient problem,\n",
    "    which can hinder training in deep networks.\n",
    "\n",
    "Max Pooling:\n",
    "    Max pooling layers were used to downsample feature maps, reducing the number of parameters and computational cost. This allowed AlexNet to \n",
    "    process larger images while maintaining efficiency. Additionally, max pooling helps improve feature generalization by making the network less\n",
    "    sensitive to small variations in the input image.\n",
    "\n",
    "Dropout Layers:\n",
    "    AlexNet introduced dropout layers, which randomly set neuron outputs to zero during training. This prevents individual neurons from becoming\n",
    "    too reliant on each other, reducing overfitting and improving the network's ability to generalize to unseen data.\n",
    "\n",
    "Larger Image Size:\n",
    "    While previous models used small input images, AlexNet utilized larger 227x227 images. This allows the network to capture more context and\n",
    "    detail in the input, leading to better feature extraction and recognition performance.\n",
    "\n",
    "Data Augmentation:\n",
    "    AlexNet employed data augmentation techniques like random cropping, flipping, and scaling to artificially increase the size and diversity of\n",
    "    the training data. This helps prevent overfitting and improves the network's robustness to variations in the real-world data.\n",
    "\n",
    "GPU Utilization:\n",
    "    AlexNet was one of the first CNNs to effectively utilize GPUs for training. GPUs offer significantly higher computational power compared to \n",
    "    CPUs, allowing for faster and more efficient training of deep networks. This enabled AlexNet to handle the increased complexity of its\n",
    "    architecture and train on a large dataset.\n",
    "\n",
    "Combination of Innovations:\n",
    "    The combined effect of these innovations was a significant leap in performance compared to previous models. Each innovation played a crucial\n",
    "    role in enabling AlexNet to learn more complex features and achieve superior image recognition accuracy.\n",
    "\n",
    "Overall:\n",
    "\n",
    "AlexNet's architectural innovations demonstrated the potential of deep learning for complex tasks like image recognition. These innovations paved \n",
    "the way for further research and development in the field, leading to the emergence of even more powerful and versatile CNN architectures that \n",
    "continue to shape the landscape of artificial intelligence today.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Discuss the role of convolutional layers, pooling layers, and fully connected layers in AlexNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''AlexNet's architecture relies on three critical types of layers, each playing a distinct role in the network's functionality:\n",
    "\n",
    "1. Convolutional Layers:\n",
    "\n",
    "    Function: Extract local features from the input image.\n",
    "    Mechanism: Apply filters (kernels) that slide across the image, detecting specific patterns and extracting features like edges, corners, and textures.\n",
    "    Impact:\n",
    "        Capture local information crucial for understanding the content of the image.\n",
    "        Increase the depth of the network, allowing for learning complex hierarchical representations.\n",
    "\n",
    "2. Pooling Layers:\n",
    "\n",
    "    Function: Reduce the dimensionality of feature maps while preserving important information.\n",
    "    Mechanism: Apply functions like max pooling or average pooling to aggregate values in small regions of the feature maps.\n",
    "    Impact:\n",
    "        Reduce computational cost by processing smaller feature maps.\n",
    "        Improve feature generalization by making the network less sensitive to small variations in the input.\n",
    "        Introduce invariance to spatial translations of features.\n",
    "\n",
    "3. Fully-Connected Layers:\n",
    "\n",
    "    Function: Combine features extracted by convolutional layers and learn complex relationships between them.\n",
    "    Mechanism: Perform linear transformations on the flattened feature maps to connect all neurons and learn global features.\n",
    "    Impact:\n",
    "        Enable the network to combine local features into higher-level representations for recognition.\n",
    "        Learn complex relationships between features that cannot be captured by convolutional layers alone.\n",
    "        Perform the final classification based on the learned features.\n",
    "\n",
    "Interplay of Layers:\n",
    "\n",
    "    Convolutional layers extract local features, while pooling layers summarize and downsample them.\n",
    "    Fully-connected layers combine these summarized features and learn complex relationships, eventually leading to the final classification decision.\n",
    "    This interplay between layers with different functionalities allows AlexNet to effectively learn hierarchical representations of visual features,\n",
    "    achieving superior performance in image recognition tasks.\n",
    "\n",
    "Overall:\n",
    "\n",
    "    Each type of layer in AlexNet plays a crucial role in the network's performance. Convolutional layers extract local features, pooling layers reduce \n",
    "    dimensionality and introduce invariance, and fully-connected layers combine features and learn complex relationships, ultimately enabling AlexNet to \n",
    "    achieve its groundbreaking performance in image recognition.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implement AlexNet using a deep learning framewofk of your choice and evaluate its performance on a dataset of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 195s 104ms/step - loss: 0.5442 - accuracy: 0.8165 - val_loss: 0.2138 - val_accuracy: 0.9529\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 195s 104ms/step - loss: 0.1877 - accuracy: 0.9533 - val_loss: 0.1163 - val_accuracy: 0.9681\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 196s 105ms/step - loss: 0.1555 - accuracy: 0.9624 - val_loss: 0.1180 - val_accuracy: 0.9705\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 197s 105ms/step - loss: 0.1324 - accuracy: 0.9680 - val_loss: 0.1799 - val_accuracy: 0.9534\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 201s 107ms/step - loss: 0.1160 - accuracy: 0.9716 - val_loss: 0.2468 - val_accuracy: 0.9295\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 202s 108ms/step - loss: 0.1233 - accuracy: 0.9696 - val_loss: 0.1316 - val_accuracy: 0.9634\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 207s 111ms/step - loss: 0.1124 - accuracy: 0.9721 - val_loss: 0.0899 - val_accuracy: 0.9743\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 208s 111ms/step - loss: 0.0900 - accuracy: 0.9773 - val_loss: 0.1374 - val_accuracy: 0.9658\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 208s 111ms/step - loss: 0.1086 - accuracy: 0.9731 - val_loss: 0.1367 - val_accuracy: 0.9731\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 206s 110ms/step - loss: 0.0900 - accuracy: 0.9778 - val_loss: 0.0911 - val_accuracy: 0.9794\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 0.0911 - accuracy: 0.9794\n",
      "Test loss: 0.09113667905330658\n",
      "Test accuracy: 0.9793999791145325\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images = train_images.astype('float32') / 255.0\n",
    "test_images = test_images.astype('float32') / 255.0\n",
    "\n",
    "# Standardize mean and standard deviation of the data\n",
    "mean = train_images.mean(axis=(0, 1, 2), keepdims=True)\n",
    "std = train_images.std(axis=(0, 1, 2), keepdims=True)\n",
    "train_images = (train_images - mean) / std\n",
    "test_images = (test_images - mean) / std\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(filters=96, kernel_size=(11, 11), strides=4, padding='same', activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D(pool_size=(3, 3), strides=2, padding='same'),\n",
    "    layers.Conv2D(filters=256, kernel_size=(5, 5), padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(3, 3), strides=2, padding='same'),\n",
    "    layers.Conv2D(filters=384, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "    layers.Conv2D(filters=384, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "    layers.Conv2D(filters=256, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(3, 3), strides=2, padding='same'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(4096, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(4096, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_images, train_labels, epochs=10, validation_data=(test_images, test_labels))\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print('Test loss:', test_loss)\n",
    "print('Test accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
